这是一个用来体验深度学习的项目，主要是对于卷积神经网络进行模型尝试和调参。以下是项目说明：

# 一、数据集：
1. MNIST 手写数字数据集
2. EMNIST 包含手写数字和字母的数据集
3. FashionMNIST 服装数据集
4. classify-leaves数据集: 取自B站up主"跟李沐学AI"，《动手学深度学习Pytorch版》视频合集里的 第二部分完结竞赛数据集，由于kaggle竞赛已经结束，删去了原始的test.csv以及sample_submission.csv文件，将train.csv里的样本进行分割，得到训练集和测试集；

有关数据集的介绍详见dataset_documentation_zh.md文档

# 二、卷积神经网络构造：
## 1.经典模型
这部分是一些经典的卷积神经网络模型。
### 1.1 LeNet
LeNet是Yann LeCun等人在1998年提出的用于手写数字识别的卷积神经网络，是最早的卷积神经网络之一。
- 第一层：卷积层(C1)，使用6个5×5的卷积核，输出特征图尺寸为28×28×6
- 第二层：池化层(S2)，使用2×2的平均池化，步长为2，输出特征图尺寸为14×14×6
- 第三层：卷积层(C3)，使用16个5×5的卷积核，输出特征图尺寸为10×10×16
- 第四层：池化层(S4)，使用2×2的平均池化，步长为2，输出特征图尺寸为5×5×16
- 第五层：全连接层(F5)，16×5×5输入，120个神经元输出
- 第六层：全连接层(F6)，120个神经元输入，84个神经元输出
- 第七层：输出层，84个神经元输入，对应类别数的输出
激活函数：Sigmoid或Tanh

### 1.2 LeNet-ReLU
LeNet-ReLU是LeNet的改进版本，将原来的Sigmoid/Tanh激活函数替换为ReLU激活函数，可以缓解梯度消失问题，加快训练速度。
- 第一层：卷积层(C1)，使用6个5×5的卷积核，输出特征图尺寸为28×28×6
- 第二层：激活函数ReLU
- 第三层：池化层(S2)，使用2×2的平均池化，步长为2，输出特征图尺寸为14×14×6
- 第四层：卷积层(C3)，使用16个5×5的卷积核，输出特征图尺寸为10×10×16
- 第五层：激活函数ReLU
- 第六层：池化层(S4)，使用2×2的平均池化，步长为2，输出特征图尺寸为5×5×16
- 第七层：全连接层(F5)，16×5×5输入，120个神经元输出
- 第八层：激活函数ReLU
- 第九层：全连接层(F6)，120个神经元输入，84个神经元输出
- 第十层：激活函数ReLU
- 第十一层：输出层，84个神经元输入，对应类别数的输出

### 1.3 LeNet-ReLU-Dropout
在LeNet-ReLU的基础上，在全连接层之间加入了Dropout层，用于防止过拟合，提高模型的泛化能力。
- 第一层：卷积层(C1)，使用6个5×5的卷积核，输出特征图尺寸为28×28×6
- 第二层：激活函数ReLU
- 第三层：池化层(S2)，使用2×2的平均池化，步长为2，输出特征图尺寸为14×14×6
- 第四层：卷积层(C3)，使用16个5×5的卷积核，输出特征图尺寸为10×10×16
- 第五层：激活函数ReLU
- 第六层：池化层(S4)，使用2×2的平均池化，步长为2，输出特征图尺寸为5×5×16
- 第七层：全连接层(F5)，16×5×5输入，120个神经元输出
- 第八层：激活函数ReLU
- 第九层：Dropout层，丢弃率默认为0.5
- 第十层：全连接层(F6)，120个神经元输入，84个神经元输出
- 第十一层：激活函数ReLU
- 第十二层：Dropout层，丢弃率默认为0.5
- 第十三层：输出层，84个神经元输入，对应类别数的输出

### 1.4 LeNet-ReLU-Dropout-BN
在LeNet-ReLU-Dropout的基础上，增加了Batch Normalization层，可以加速训练过程，提高模型的稳定性。
- 第一层：卷积层(C1)，使用6个5×5的卷积核，输出特征图尺寸为28×28×6
- 第二层：批归一化层(BN)
- 第三层：激活函数ReLU
- 第四层：池化层(S2)，使用2×2的平均池化，步长为2，输出特征图尺寸为14×14×6
- 第五层：卷积层(C3)，使用16个5×5的卷积核，输出特征图尺寸为10×10×16
- 第六层：批归一化层(BN)
- 第七层：激活函数ReLU
- 第八层：池化层(S4)，使用2×2的平均池化，步长为2，输出特征图尺寸为5×5×16
- 第九层：全连接层(F5)，16×5×5输入，120个神经元输出
- 第十层：激活函数ReLU
- 第十一层：Dropout层，丢弃率默认为0.5
- 第十二层：全连接层(F6)，120个神经元输入，84个神经元输出
- 第十三层：激活函数ReLU
- 第十四层：Dropout层，丢弃率默认为0.5
- 第十五层：输出层，84个神经元输入，对应类别数的输出

### 1.5 ResNet
ResNet（Residual Network）是微软研究院的Kaiming He等人在2015年提出的深度残差网络，通过引入残差连接解决了深度网络训练困难的问题。
- 第一层：卷积层，使用7×7的卷积核，64个输出通道，步长为2，填充为3
- 第二层：批归一化层(BN)
- 第三层：激活函数ReLU
- 第四层：最大池化层，3×3的池化窗口，步长为2，填充为1
- 第五层：残差块层(layer1)，包含指定数量的残差块
- 第六层：残差块层(layer2)，包含指定数量的残差块，具有下采样
- 第七层：残差块层(layer3)，包含指定数量的残差块，具有下采样
- 第八层：残差块层(layer4)，包含指定数量的残差块，具有下采样
- 第九层：全局平均池化层，输出大小为1×1
- 第十层：全连接层，输出对应类别数

残差块结构：
- 主路径：卷积层→批归一化层→激活函数ReLU→卷积层→批归一化层
- 捷径连接：如果输入输出维度不一致，则使用1×1卷积调整维度
- 最终输出：主路径输出与捷径连接输出相加后通过ReLU激活函数

### 1.6 ResNet-18
ResNet-18是ResNet系列中的一种，包含18层有权重的层，使用基本的残差块（BasicBlock）构建，结构相对简单但效果良好。
- 第一层：卷积层，使用7×7的卷积核，64个输出通道，步长为2，填充为3
- 第二层：批归一化层(BN)
- 第三层：激活函数ReLU
- 第四层：最大池化层，3×3的池化窗口，步长为2，填充为1
- 第五层：残差块层(layer1)，包含2个残差块，每个块有2个3×3卷积层，输出通道64
- 第六层：残差块层(layer2)，包含2个残差块，每个块有2个3×3卷积层，输出通道128，具有下采样
- 第七层：残差块层(layer3)，包含2个残差块，每个块有2个3×3卷积层，输出通道256，具有下采样
- 第八层：残差块层(layer4)，包含2个残差块，每个块有2个3×3卷积层，输出通道512，具有下采样
- 第九层：全局平均池化层，输出大小为1×1
- 第十层：全连接层，输出对应类别数(512→num_classes)

### 1.7 ResNet-34
ResNet-34是ResNet系列中的另一种，包含34层有权重的层，同样使用基本的残差块构建，比ResNet-18更深。
- 第一层：卷积层，使用7×7的卷积核，64个输出通道，步长为2，填充为3
- 第二层：批归一化层(BN)
- 第三层：激活函数ReLU
- 第四层：最大池化层，3×3的池化窗口，步长为2，填充为1
- 第五层：残差块层(layer1)，包含3个残差块，每个块有2个3×3卷积层，输出通道64
- 第六层：残差块层(layer2)，包含4个残差块，每个块有2个3×3卷积层，输出通道128，具有下采样
- 第七层：残差块层(layer3)，包含6个残差块，每个块有2个3×3卷积层，输出通道256，具有下采样
- 第八层：残差块层(layer4)，包含3个残差块，每个块有2个3×3卷积层，输出通道512，具有下采样
- 第九层：全局平均池化层，输出大小为1×1
- 第十层：全连接层，输出对应类别数(512→num_classes)

### 1.8 AlexNet
AlexNet是由Alex Krizhevsky等人在2012年提出的深度卷积神经网络，在ImageNet竞赛中取得了突破性成果。它包含5个卷积层和3个全连接层，使用ReLU激活函数和Dropout技术。
- 第一层：卷积层，使用96个11×11的卷积核，步长为4，填充为2
- 第二层：激活函数ReLU
- 第三层：最大池化层，3×3的池化窗口，步长为2
- 第四层：卷积层，使用256个5×5的卷积核，填充为2
- 第五层：激活函数ReLU
- 第六层：最大池化层，3×3的池化窗口，步长为2
- 第七层：卷积层，使用384个3×3的卷积核，填充为1
- 第八层：激活函数ReLU
- 第九层：卷积层，使用384个3×3的卷积核，填充为1
- 第十层：激活函数ReLU
- 第十一层：卷积层，使用256个3×3的卷积核，填充为1
- 第十二层：激活函数ReLU
- 第十三层：最大池化层，3×3的池化窗口，步长为2
- 第十四层：自适应平均池化层，输出大小为6×6
- 第十五层：Dropout层，丢弃率为0.5
- 第十六层：全连接层，输出4096个神经元
- 第十七层：激活函数ReLU
- 第十八层：Dropout层，丢弃率为0.5
- 第十九层：全连接层，输出4096个神经元
- 第二十层：激活函数ReLU
- 第二十一层：全连接层，输出对应类别数

### 1.9 VGG
VGG是由牛津大学Visual Geometry Group提出的卷积神经网络，以其简洁统一的结构而闻名。VGG通过使用多个小卷积核（3×3）堆叠来代替大卷积核，加深了网络深度。
- 第一组：卷积层(64个3×3卷积核)→ReLU→卷积层(64个3×3卷积核)→ReLU→最大池化层(2×2)
- 第二组：卷积层(128个3×3卷积核)→ReLU→卷积层(128个3×3卷积核)→ReLU→最大池化层(2×2)
- 第三组：卷积层(256个3×3卷积核)→ReLU→卷积层(256个3×3卷积核)→ReLU→卷积层(256个3×3卷积核)→ReLU→最大池化层(2×2)
- 第四组：卷积层(512个3×3卷积核)→ReLU→卷积层(512个3×3卷积核)→ReLU→卷积层(512个3×3卷积核)→ReLU→最大池化层(2×2)
- 第五组：卷积层(512个3×3卷积核)→ReLU→卷积层(512个3×3卷积核)→ReLU→卷积层(512个3×3卷积核)→ReLU→最大池化层(2×2)
- 自适应平均池化层，输出大小为7×7
- 全连接层(512×7×7→4096)→ReLU→Dropout→全连接层(4096→4096)→ReLU→Dropout→全连接层(4096→num_classes)

### 1.10 NiN
NiN（Network in Network）是由Min Lin等人在2013年提出的网络结构，创新性地在卷积层内部使用多层感知机（MLP）来替代传统的卷积操作，通过全局平均池化替代全连接层来减少参数。
- 第一层：NiN块，输入通道1，输出通道96，卷积核11×11，步长4，无填充→最大池化层(3×3，步长2)
- 第二层：NiN块，输入通道96，输出通道256，卷积核5×5，步长1，填充2→最大池化层(3×3，步长2)
- 第三层：NiN块，输入通道256，输出通道384，卷积核3×3，步长1，填充1→最大池化层(3×3，步长2)→Dropout层(0.5)
- 第四层：NiN块，输入通道384，输出通道num_classes，卷积核3×3，步长1，填充1
- 全局平均池化层，输出大小为1×1
- 展平层，输出类别数

NiN块结构：
- 卷积层→ReLU→1×1卷积层→ReLU→1×1卷积层→ReLU

### 特别说明
"## 1.经典模型" 节文档由通义灵码 VSCode 插件中的 qwen3-coder 模型生成。

## 2. 自定义模型

### 实现方法
1. 创建一个主FlexibleCNN类：这个类继承自torch.nn.Module。它的__init__方法接收一个config字典作为参数。
2. 解析配置：在__init__方法中，我们会遍历config中定义网络结构的部分（例如一个名为layers_config的列表）。
3. 使用nn.ModuleList或nn.Sequential：
    对于特征提取部分（卷积、池化等），我们将动态创建的层添加到一个nn.ModuleList中。这提供了最大的灵活性，因为我们可以在forward方法中自定义它们的调用顺序（例如，实现残差连接）。
    对于分类器部分（全连接层），通常是线性的序列，直接使用nn.Sequential来构建。
4. 自动计算维度：一个常见的难点是从卷积/池化层过渡到全连接层时，如何确定flatten之后的大小。我们将在模型初始化时，通过创建一个虚拟输入张量（dummy tensor）并让它通过所有特征提取层，来自动计算出这个维度。这样，用户在配置全连接层时就无需手动计算了。
5. 将训练逻辑分离：将模型的定义（FlexibleCNN类）与训练和评估的循环代码分离开。这样可以使代码结构更清晰。

### 可调节参数设计
将所有可调参数都集中在一个config字典中，方便管理和修改。
1. 数据与设备 (Data & Device)
dataset: 'CIFAR10' 或 'MNIST' 等，用于自动加载数据。
data_path: 数据集存放路径。
device: 'cuda' 或 'cpu'，自动检测。
2. 模型架构 (Model Architecture)
    input_shape: 输入图像的形状，例如 (3, 32, 32) for CIFAR-10。用于自动计算维度。
    num_classes: 分类任务的类别数。
    feature_extractor_config: 一个列表，定义了特征提取部分的每一层。列表中的每个元素都是一个字典，描述一个“块”。
    卷积块 (Convolutional Block):
        type: 'conv'
        out_channels: 输出通道数
        kernel_size: 卷积核大小
        stride: 步长
        padding: 填充
        activation: 激活函数，如 'relu', 'leaky_relu'
        use_batchnorm: True或False，是否使用批量归一化
    池化块 (Pooling Block):
        type: 'pool'
        pool_type: 池化类型，如 'max', 'avg'
        kernel_size: 池化核大小
        stride: 步长
    残差块 (Residual Block):
        BasicBlock (基础块)
            结构: 两个 3x3 的卷积层，两个卷积层的输出通道数是相同的。
            用途: 用于较浅的ResNet模型，如ResNet-18, ResNet-34。
            通道扩展: 输入通道数和输出通道数通常是相同的（expansion = 1）。当需要下采样时，第一个卷积层的stride=2，并且快捷连接（shortcut）需要一个1x1卷积来匹配维度。
            type: 'basic_block'
            out_channels: 块的输出通道数
            stride: 第一个卷积层的步长（通常为1或2，2用于下采样）
        Bottleneck (瓶颈块)
            结构: 三个卷积层：1x1（降维） -> 3x3（特征提取） -> 1x1（升维）。
            用途: 用于较深的ResNet模型，如ResNet-50, ResNet-101。这种设计在保持较大感受野的同时，显著减少了参数量和计算量。
            通道扩展: 输出通道数是中间3x3卷积层通道数的4倍（expansion = 4）。例如，如果指定out_channels=64，那么这个块的最终输出通道数将是64 * 4 = 256。这个设计非常关键。
            type: 'bottleneck'
            out_channels: 中间3x3卷积层的通道数。最终输出通道数将是这个值的4倍。
            stride: 中间3x3卷积层的步长（通常为1或2）

    classifier_config: 定义分类器（全连接层）部分。
    hidden_layers: 一个列表，包含每个隐藏层的神经元数量，例如 [512, 256]。
    dropout_rate: 在全连接层之间使用的Dropout比率。
    activation: 激活函数。
3. 训练超参数 (Training Hyperparameters)
    training_params:
    optimizer: 优化器类型, 如 'Adam', 'SGD'
    loss_function: 损失函数, 如 'CrossEntropyLoss'
    learning_rate: 学习率
    epochs: 训练轮数
    batch_size: 批处理大小
