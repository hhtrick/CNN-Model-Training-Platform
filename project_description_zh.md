这是一个用来体验深度学习的项目，主要是对于卷积神经网络进行模型尝试和调参。以下是项目说明：

# 一、数据集：
1. MNIST 手写数字数据集
2. EMNIST 包含手写数字和字母的数据集
3. FashionMNIST 服装数据集
4. classify-leaves数据集: 取自B站up主"跟李沐学AI"，《动手学深度学习Pytorch版》视频合集里的 第二部分完结竞赛数据集，由于kaggle竞赛已经结束，删去了原始的test.csv以及sample_submission.csv文件，将train.csv里的样本进行分割，得到训练集和测试集；

有关数据集的介绍详见dataset_documentation_zh.md文档

# 二、卷积神经网络构造：
## 1.经典模型
这部分是一些经典的卷积神经网络模型。
### 1.1 LeNet
LeNet是Yann LeCun等人在1998年提出的用于手写数字识别的卷积神经网络，是最早的卷积神经网络之一。
- 第一层：卷积层(C1)，使用6个5×5的卷积核，输出特征图尺寸为28×28×6
- 第二层：池化层(S2)，使用2×2的平均池化，步长为2，输出特征图尺寸为14×14×6
- 第三层：卷积层(C3)，使用16个5×5的卷积核，输出特征图尺寸为10×10×16
- 第四层：池化层(S4)，使用2×2的平均池化，步长为2，输出特征图尺寸为5×5×16
- 第五层：全连接层(F5)，16×5×5输入，120个神经元输出
- 第六层：全连接层(F6)，120个神经元输入，84个神经元输出
- 第七层：输出层，84个神经元输入，对应类别数的输出
激活函数：Sigmoid或Tanh

### 1.2 LeNet-ReLU
LeNet-ReLU是LeNet的改进版本，将原来的Sigmoid/Tanh激活函数替换为ReLU激活函数，可以缓解梯度消失问题，加快训练速度。
- 第一层：卷积层(C1)，使用6个5×5的卷积核，输出特征图尺寸为28×28×6
- 第二层：激活函数ReLU
- 第三层：池化层(S2)，使用2×2的平均池化，步长为2，输出特征图尺寸为14×14×6
- 第四层：卷积层(C3)，使用16个5×5的卷积核，输出特征图尺寸为10×10×16
- 第五层：激活函数ReLU
- 第六层：池化层(S4)，使用2×2的平均池化，步长为2，输出特征图尺寸为5×5×16
- 第七层：全连接层(F5)，16×5×5输入，120个神经元输出
- 第八层：激活函数ReLU
- 第九层：全连接层(F6)，120个神经元输入，84个神经元输出
- 第十层：激活函数ReLU
- 第十一层：输出层，84个神经元输入，对应类别数的输出

### 1.3 LeNet-ReLU-Dropout
在LeNet-ReLU的基础上，在全连接层之间加入了Dropout层，用于防止过拟合，提高模型的泛化能力。
- 第一层：卷积层(C1)，使用6个5×5的卷积核，输出特征图尺寸为28×28×6
- 第二层：激活函数ReLU
- 第三层：池化层(S2)，使用2×2的平均池化，步长为2，输出特征图尺寸为14×14×6
- 第四层：卷积层(C3)，使用16个5×5的卷积核，输出特征图尺寸为10×10×16
- 第五层：激活函数ReLU
- 第六层：池化层(S4)，使用2×2的平均池化，步长为2，输出特征图尺寸为5×5×16
- 第七层：全连接层(F5)，16×5×5输入，120个神经元输出
- 第八层：激活函数ReLU
- 第九层：Dropout层，丢弃率默认为0.5
- 第十层：全连接层(F6)，120个神经元输入，84个神经元输出
- 第十一层：激活函数ReLU
- 第十二层：Dropout层，丢弃率默认为0.5
- 第十三层：输出层，84个神经元输入，对应类别数的输出

### 1.4 LeNet-ReLU-Dropout-BN
在LeNet-ReLU-Dropout的基础上，增加了Batch Normalization层，可以加速训练过程，提高模型的稳定性。
- 第一层：卷积层(C1)，使用6个5×5的卷积核，输出特征图尺寸为28×28×6
- 第二层：批归一化层(BN)
- 第三层：激活函数ReLU
- 第四层：池化层(S2)，使用2×2的平均池化，步长为2，输出特征图尺寸为14×14×6
- 第五层：卷积层(C3)，使用16个5×5的卷积核，输出特征图尺寸为10×10×16
- 第六层：批归一化层(BN)
- 第七层：激活函数ReLU
- 第八层：池化层(S4)，使用2×2的平均池化，步长为2，输出特征图尺寸为5×5×16
- 第九层：全连接层(F5)，16×5×5输入，120个神经元输出
- 第十层：激活函数ReLU
- 第十一层：Dropout层，丢弃率默认为0.5
- 第十二层：全连接层(F6)，120个神经元输入，84个神经元输出
- 第十三层：激活函数ReLU
- 第十四层：Dropout层，丢弃率默认为0.5
- 第十五层：输出层，84个神经元输入，对应类别数的输出

### 1.5 ResNet
ResNet（Residual Network）是微软研究院的Kaiming He等人在2015年提出的深度残差网络，通过引入残差连接解决了深度网络训练困难的问题。
- 第一层：卷积层，使用7×7的卷积核，64个输出通道，步长为2，填充为3
- 第二层：批归一化层(BN)
- 第三层：激活函数ReLU
- 第四层：最大池化层，3×3的池化窗口，步长为2，填充为1
- 第五层：残差块层(layer1)，包含指定数量的残差块
- 第六层：残差块层(layer2)，包含指定数量的残差块，具有下采样
- 第七层：残差块层(layer3)，包含指定数量的残差块，具有下采样
- 第八层：残差块层(layer4)，包含指定数量的残差块，具有下采样
- 第九层：全局平均池化层，输出大小为1×1
- 第十层：全连接层，输出对应类别数

残差块结构：
- 主路径：卷积层→批归一化层→激活函数ReLU→卷积层→批归一化层
- 捷径连接：如果输入输出维度不一致，则使用1×1卷积调整维度
- 最终输出：主路径输出与捷径连接输出相加后通过ReLU激活函数

### 1.6 ResNet-18
ResNet-18是ResNet系列中的一种，包含18层有权重的层，使用基本的残差块（BasicBlock）构建，结构相对简单但效果良好。
- 第一层：卷积层，使用7×7的卷积核，64个输出通道，步长为2，填充为3
- 第二层：批归一化层(BN)
- 第三层：激活函数ReLU
- 第四层：最大池化层，3×3的池化窗口，步长为2，填充为1
- 第五层：残差块层(layer1)，包含2个残差块，每个块有2个3×3卷积层，输出通道64
- 第六层：残差块层(layer2)，包含2个残差块，每个块有2个3×3卷积层，输出通道128，具有下采样
- 第七层：残差块层(layer3)，包含2个残差块，每个块有2个3×3卷积层，输出通道256，具有下采样
- 第八层：残差块层(layer4)，包含2个残差块，每个块有2个3×3卷积层，输出通道512，具有下采样
- 第九层：全局平均池化层，输出大小为1×1
- 第十层：全连接层，输出对应类别数(512→num_classes)

### 1.7 ResNet-34
ResNet-34是ResNet系列中的另一种，包含34层有权重的层，同样使用基本的残差块构建，比ResNet-18更深。
- 第一层：卷积层，使用7×7的卷积核，64个输出通道，步长为2，填充为3
- 第二层：批归一化层(BN)
- 第三层：激活函数ReLU
- 第四层：最大池化层，3×3的池化窗口，步长为2，填充为1
- 第五层：残差块层(layer1)，包含3个残差块，每个块有2个3×3卷积层，输出通道64
- 第六层：残差块层(layer2)，包含4个残差块，每个块有2个3×3卷积层，输出通道128，具有下采样
- 第七层：残差块层(layer3)，包含6个残差块，每个块有2个3×3卷积层，输出通道256，具有下采样
- 第八层：残差块层(layer4)，包含3个残差块，每个块有2个3×3卷积层，输出通道512，具有下采样
- 第九层：全局平均池化层，输出大小为1×1
- 第十层：全连接层，输出对应类别数(512→num_classes)

### 1.8 AlexNet
AlexNet是由Alex Krizhevsky等人在2012年提出的深度卷积神经网络，在ImageNet竞赛中取得了突破性成果。它包含5个卷积层和3个全连接层，使用ReLU激活函数和Dropout技术。
- 第一层：卷积层，使用96个11×11的卷积核，步长为4，填充为2
- 第二层：激活函数ReLU
- 第三层：最大池化层，3×3的池化窗口，步长为2
- 第四层：卷积层，使用256个5×5的卷积核，填充为2
- 第五层：激活函数ReLU
- 第六层：最大池化层，3×3的池化窗口，步长为2
- 第七层：卷积层，使用384个3×3的卷积核，填充为1
- 第八层：激活函数ReLU
- 第九层：卷积层，使用384个3×3的卷积核，填充为1
- 第十层：激活函数ReLU
- 第十一层：卷积层，使用256个3×3的卷积核，填充为1
- 第十二层：激活函数ReLU
- 第十三层：最大池化层，3×3的池化窗口，步长为2
- 第十四层：自适应平均池化层，输出大小为6×6
- 第十五层：Dropout层，丢弃率为0.5
- 第十六层：全连接层，输出4096个神经元
- 第十七层：激活函数ReLU
- 第十八层：Dropout层，丢弃率为0.5
- 第十九层：全连接层，输出4096个神经元
- 第二十层：激活函数ReLU
- 第二十一层：全连接层，输出对应类别数

### 1.9 VGG
VGG是由牛津大学Visual Geometry Group提出的卷积神经网络，以其简洁统一的结构而闻名。VGG通过使用多个小卷积核（3×3）堆叠来代替大卷积核，加深了网络深度。
- 第一组：卷积层(64个3×3卷积核)→ReLU→卷积层(64个3×3卷积核)→ReLU→最大池化层(2×2)
- 第二组：卷积层(128个3×3卷积核)→ReLU→卷积层(128个3×3卷积核)→ReLU→最大池化层(2×2)
- 第三组：卷积层(256个3×3卷积核)→ReLU→卷积层(256个3×3卷积核)→ReLU→卷积层(256个3×3卷积核)→ReLU→最大池化层(2×2)
- 第四组：卷积层(512个3×3卷积核)→ReLU→卷积层(512个3×3卷积核)→ReLU→卷积层(512个3×3卷积核)→ReLU→最大池化层(2×2)
- 第五组：卷积层(512个3×3卷积核)→ReLU→卷积层(512个3×3卷积核)→ReLU→卷积层(512个3×3卷积核)→ReLU→最大池化层(2×2)
- 自适应平均池化层，输出大小为7×7
- 全连接层(512×7×7→4096)→ReLU→Dropout→全连接层(4096→4096)→ReLU→Dropout→全连接层(4096→num_classes)

### 1.10 NiN
NiN（Network in Network）是由Min Lin等人在2013年提出的网络结构，创新性地在卷积层内部使用多层感知机（MLP）来替代传统的卷积操作，通过全局平均池化替代全连接层来减少参数。
- 第一层：NiN块，输入通道1，输出通道96，卷积核11×11，步长4，无填充→最大池化层(3×3，步长2)
- 第二层：NiN块，输入通道96，输出通道256，卷积核5×5，步长1，填充2→最大池化层(3×3，步长2)
- 第三层：NiN块，输入通道256，输出通道384，卷积核3×3，步长1，填充1→最大池化层(3×3，步长2)→Dropout层(0.5)
- 第四层：NiN块，输入通道384，输出通道num_classes，卷积核3×3，步长1，填充1
- 全局平均池化层，输出大小为1×1
- 展平层，输出类别数

NiN块结构：
- 卷积层→ReLU→1×1卷积层→ReLU→1×1卷积层→ReLU

### 特别说明
"## 1.经典模型" 节文档由通义灵码 VSCode 插件中的 qwen3-coder 模型生成。

## 2. 自定义模型

待开发
